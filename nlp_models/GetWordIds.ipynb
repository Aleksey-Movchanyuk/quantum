{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = gensim.corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to convert raw text to word ids representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_corpora(raw_text):\n",
    "    sents = nltk.tokenize.sent_tokenize(raw_text)\n",
    "    tmp = []\n",
    "    for s in sents:\n",
    "        words = nltk.tokenize.wordpunct_tokenize(s)\n",
    "        tmp.append(words)\n",
    "    \n",
    "    corpora.add_documents(tmp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_text_to_word_ids(raw_text):\n",
    "    \n",
    "    sents = nltk.tokenize.sent_tokenize(raw_text)\n",
    "    res = []\n",
    "    for s in sents:\n",
    "        words = nltk.tokenize.wordpunct_tokenize(s)\n",
    "        for w in words:\n",
    "            wid = corpora.token2id[w]\n",
    "            res.append(wid)\n",
    "            #print('%s -> %d'%(w, wid))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each file in raw_texts folder get a word ids representation and build a carpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#files = os.listdir('./raw_texts/')\n",
    "#files = ['test.txt', 'wonderland.txt']\n",
    "files = ['test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: test.txt\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    print('Processing: ' + f)\n",
    "    raw_text = open('./raw_texts/' + f).read()\n",
    "    update_corpora(raw_text)\n",
    "    r = convert_text_to_word_ids(raw_text)\n",
    "    #out_file = open('./data/'+f+'.dat', 'w')\n",
    "    #json.dump(r, json_file)\n",
    "    #pickle.dump(r, out_file)\n",
    "    #print(r)\n",
    "    with open('./data/'+f+'.dat', 'wb') as f:\n",
    "        pickle.dump(r, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = list(corpora.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 'doing'),\n",
       " (5, 'seven'),\n",
       " (22, 'How'),\n",
       " (11, 'brown'),\n",
       " (6, 'six'),\n",
       " (9, 'eight'),\n",
       " (15, 'fox'),\n",
       " (18, 'lazy'),\n",
       " (21, 'are'),\n",
       " (8, 'four'),\n",
       " (23, 'you'),\n",
       " (0, 'two'),\n",
       " (1, 'ten'),\n",
       " (3, 'five'),\n",
       " (20, '?'),\n",
       " (7, 'nine'),\n",
       " (14, 'dog'),\n",
       " (25, 'What'),\n",
       " (4, 'three'),\n",
       " (10, '.'),\n",
       " (2, 'One'),\n",
       " (12, 'The'),\n",
       " (19, 'a'),\n",
       " (16, 'over'),\n",
       " (13, 'quick'),\n",
       " (17, 'jumps')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.save('./data/corpora.dat')\n",
    "corpora.save_as_text('./data/corpora.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpora: 26\n"
     ]
    }
   ],
   "source": [
    "print('Number of words in corpora: %d'%(len(corpora)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = list(corpora.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora.id2token[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
