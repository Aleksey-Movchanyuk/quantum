{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'test.txt'\n",
    "#filename = 'en_US.blogs.txt'\n",
    "#trainset_prefix = 'part001'\n",
    "trainset_prefix = ''\n",
    "\n",
    "IN_SEQ_LENGTH = 3\n",
    "OUT_SEQ_LENGTH = 1\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_LAYER_1 = 1024\n",
    "MODEL_NAME = 'nn_3_in_1_lstm_1_out'\n",
    "\n",
    "N_SAMPLES = 10000000\n",
    "N_EPOCH = 4\n",
    "N_BATCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "META_EMPTY = '<<<!EMP!>>>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpora: 14\n"
     ]
    }
   ],
   "source": [
    "corpora = gensim.corpora.Dictionary.load('./data/'+filename+'.corpora.dat')\n",
    "vocab_size = len(corpora)\n",
    "print('Number of words in corpora: %d'%(vocab_size))\n",
    "tmp = list(corpora.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=IN_SEQ_LENGTH) )\n",
    "model.add( LSTM(HIDDEN_LAYER_1, return_sequences=False) )\n",
    "\n",
    "model.add( Dense(output_dim=vocab_size, activation='softmax') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_9 (Embedding)          (None, 3, 64)         896         embedding_input_9[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                    (None, 1024)          4460544     embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 14)            14350       lstm_9[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 4475790\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = './data/' + MODEL_NAME + '.json'\n",
    "with open(fname, mode='w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train set...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading train set...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('./data/' + filename + '.' + str(IN_SEQ_LENGTH) + '_in_' + str(OUT_SEQ_LENGTH) + '_out.train_X.csv')\n",
    "train_Y = pd.read_csv('./data/' + filename + '.' + str(IN_SEQ_LENGTH) + '_in_' + str(OUT_SEQ_LENGTH) + '_out.train_Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trancate dataset\n",
    "N_SAMPLES = min(N_SAMPLES, train_X.shape[0])\n",
    "\n",
    "train_X = train_X.loc[:N_SAMPLES,:]\n",
    "train_Y = train_Y.loc[:N_SAMPLES,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set X, Y:\n",
      "(39, 3) (39, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train set X, Y:')\n",
    "print( train_X.shape, train_Y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:]\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield (X_batch, y_batch)\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Epoch 00000: loss improved from inf to 2.63966, saving model to ./data/test.txt.nn_3_in_1_lstm_1_out.weights-epoch-00-loss-2.64-acc-0.08.hdf5\n",
      "39/39 [==============================] - 0s - loss: 2.6397 - acc: 0.0769\n",
      "Epoch 2/4\n",
      "Epoch 00001: loss improved from 2.63966 to 2.61565, saving model to ./data/test.txt.nn_3_in_1_lstm_1_out.weights-epoch-01-loss-2.62-acc-0.92.hdf5\n",
      "39/39 [==============================] - 0s - loss: 2.6156 - acc: 0.9231\n",
      "Epoch 3/4\n",
      "Epoch 00002: loss improved from 2.61565 to 2.59095, saving model to ./data/test.txt.nn_3_in_1_lstm_1_out.weights-epoch-02-loss-2.59-acc-1.00.hdf5\n",
      "39/39 [==============================] - 0s - loss: 2.5909 - acc: 1.0000\n",
      "Epoch 4/4\n",
      "Epoch 00003: loss improved from 2.59095 to 2.56434, saving model to ./data/test.txt.nn_3_in_1_lstm_1_out.weights-epoch-03-loss-2.56-acc-0.92.hdf5\n",
      "39/39 [==============================] - 0s - loss: 2.5643 - acc: 0.9231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f54db38b978>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkname = './data/' + filename + '.' + MODEL_NAME + '.weights.hdf5'\n",
    "checkname  = './data/' + filename + '.' + MODEL_NAME + '.weights-epoch-{epoch:02d}-loss-{loss:.2f}-acc-{acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkname, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit_generator(generator = batch_generator(train_X.as_matrix(), train_Y.as_matrix(), N_BATCH),\n",
    "                    nb_epoch = N_EPOCH,\n",
    "                    samples_per_epoch = train_X.shape[0],\n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose = 1)\n",
    "\n",
    "#model.fit(train_X.as_matrix(), train_Y.as_matrix(), batch_size=500, nb_epoch=N_EPOCH, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#scores = model.evaluate(train_X, train_Y, verbose=0)\n",
    "#print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
